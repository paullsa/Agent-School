{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c978a60",
   "metadata": {},
   "source": [
    "### LangChain Reasoning and Synthesis Demonstration\n",
    "---\n",
    "This notebook demonstrates the use of LangChain to perform reasoning tasks using local models via Ollama.\n",
    "We will break down a complex question into logical steps, retrieve relevant information for each step, and synthesize a final answer.\n",
    "\n",
    "The question we're going to answer is \"Who was the U.S. president during the moon landing, and what was his policy on space exploration?\"\n",
    "\n",
    "Let's start by installing any packages we need "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721dc7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required libraries from requirements.txt \n",
    "# pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4c9e19",
   "metadata": {},
   "source": [
    "Import any modules required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766a0570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `try` block ensures any imports are loaded successfully, and if there is an error during import, it will be caught and printed. This is useful for debugging and ensuring that the necessary libraries are available before running the main code.\n",
    "\n",
    "try:\n",
    "    # Import OllamaLLM, which allows using locally-run language models with LangChain\n",
    "    from langchain_ollama import OllamaLLM\n",
    "\n",
    "    # Import the PromptTemplate class used to create reusable prompt structures for LLMs\n",
    "    from langchain.prompts import PromptTemplate\n",
    "\n",
    "    # Import RunnableMap, which enables chaining multiple LangChain components together in a pipeline\n",
    "    from langchain_core.runnables import RunnableMap\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"Import error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error during imports: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe98181",
   "metadata": {},
   "source": [
    "### Load Local Reasoning and Synthesis Models\n",
    "Here, we initialize two models using Ollama:\n",
    "- **Reasoning Model (`phi4-mini`)**: Used to break down questions into logical steps.\n",
    "- **Synthesis Model (`Gemma3:1b`)**: Used to synthesize a final answer based on retrieved information.\n",
    "The `try` block ensures that the models are loaded successfully, and if there is an error (e.g., Ollama is not running), it provides helpful debugging information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a17818",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    reasoning_llm = OllamaLLM(model=\"phi4-mini\")\n",
    "    synthesis_llm = OllamaLLM(model=\"Gemma3:1b\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Failed to connect to Ollama or load model 'phi4-mini'.\")\n",
    "    print(\"üí° Make sure Ollama is running and the model is available:\")\n",
    "    print(\"    ollama run phi4-mini\")\n",
    "    print(f\"Error details: {e}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab44b631",
   "metadata": {},
   "source": [
    "### Define Reasoning Prompt\n",
    "We define a prompt template to guide the reasoning model. The template instructs the model to break down a given question into logical steps.\n",
    "The `PromptTemplate.from_template` method allows us to create a reusable template with placeholders (e.g., `{question}`) that can be dynamically filled with input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1081e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a reasoning assistant. Break the following question into logical steps. Use the provided information to answer the question:\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Step-by-step breakdown:\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574fa5f4",
   "metadata": {},
   "source": [
    "### Chain Prompt and Model\n",
    "We use LangChain's Expression Language to chain the reasoning prompt with the reasoning model. This creates a pipeline where the input question is first processed by the prompt and then passed to the reasoning model for step-by-step breakdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22c7281",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_chain = reasoning_prompt | reasoning_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ed0590",
   "metadata": {},
   "source": [
    "### Invoke Reasoning Chain\n",
    "We provide a question to the reasoning chain and invoke it to generate a step-by-step breakdown of the question.\n",
    "The `invoke` method processes the input through the chain and returns the model's response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfb2323",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who was the U.S. president during the moon landing, and what was his policy on space exploration?\"\n",
    "response = step_chain.invoke({\"question\": question})\n",
    "print(\"üí° Reasoning steps from the model:\\n\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b48dc0",
   "metadata": {},
   "source": [
    "### Call the Wikidpedia API\n",
    "This code uses LangChain tools to interact with Wikipedia. It breaks a question into reasoning steps, retrieves facts from Wikipedia for each step, and displays the results.\n",
    "\n",
    "### Key Steps\n",
    "\n",
    "1. **Initialize Tools**: Set up `WikipediaAPIWrapper` and `WikipediaQueryRun` for Wikipedia lookups.\n",
    "2. **Process Question**: Use `step_chain.invoke()` to break the question into steps.\n",
    "3. **Retrieve Facts**: Look up each step using the Wikipedia tool, handle errors, and collect results.\n",
    "4. **Display Results**: Combine retrieved facts into a single output and print them.\n",
    "\n",
    "### Parse and Retrieve Information\n",
    "In this step, we extract reasoning steps from the output of step_chain.invoke() using a regular expression (re.findall). The regular expression is designed to match the lines containing the numbered steps of reasoning.\n",
    "\n",
    "For each extracted step, we use the wiki_tool.run() method to perform a Wikipedia lookup. If any errors occur during the lookup, they are caught and handled gracefully by capturing the exception. The result of each lookup (whether it‚Äôs a fact from Wikipedia or an error message) is stored in a list called facts.\n",
    "\n",
    "Once all the steps have been processed, the individual facts are combined into a single string (combined_facts) and printed for further review or processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99923252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Correct setup for Wikipedia Tool ---\n",
    "from langchain.tools.wikipedia.tool import WikipediaQueryRun\n",
    "from langchain_community.utilities.wikipedia import WikipediaAPIWrapper\n",
    "\n",
    "wiki_api = WikipediaAPIWrapper(top_k_results=1, lang=\"en\")\n",
    "wiki_tool = WikipediaQueryRun(api_wrapper=wiki_api)\n",
    "\n",
    "# --- Use Wikipedia for each reasoning step ---\n",
    "\n",
    "steps_text = step_chain.invoke({\"question\": question})\n",
    "\n",
    "print(\"\\nüß† Reasoning steps:\\n\")\n",
    "print(steps_text)\n",
    "\n",
    "# Extract lines that look like steps\n",
    "step_lines = re.findall(r\"\\d+\\.\\s+(.*)\", steps_text)\n",
    "facts = []\n",
    "\n",
    "for step in step_lines:\n",
    "    print(f\"üîç Looking up: {step}\")\n",
    "    try:\n",
    "        result = wiki_tool.run(step)\n",
    "    except Exception as e:\n",
    "        result = f\"Error retrieving from Wikipedia: {e}\"\n",
    "    facts.append(f\"- {step.strip()}: {result}\")\n",
    "\n",
    "combined_facts = \"\\n\".join(facts)\n",
    "\n",
    "print(\"\\nüìö Retrieved facts from Wikipedia:\\n\")\n",
    "print(combined_facts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5cfd6d",
   "metadata": {},
   "source": [
    "### Define Synthesis Prompt\n",
    "We define a prompt template to guide the synthesis model. The template combines the question and retrieved facts to generate a complete answer.\n",
    "The `PromptTemplate.from_template` method is used to create a structured template with placeholders for the question and facts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880d10b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesis_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Based on the following question and information, write a complete answer.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Information:\n",
    "{facts}\n",
    "\n",
    "Answer:\n",
    "\"\"\")\n",
    "synthesis_chain = synthesis_prompt | synthesis_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f293d3cf",
   "metadata": {},
   "source": [
    "### Generate Final Answer\n",
    "We invoke the synthesis chain to generate the final answer based on the question and the retrieved facts.\n",
    "The `invoke` method processes the input through the synthesis chain and returns the model's response, which is the final synthesized answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48fd9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_answer = synthesis_chain.invoke({\n",
    "    \"question\": question,\n",
    "    \"facts\": combined_facts\n",
    "})\n",
    "print(\"\\nüß† Final answer from synthesis model:\\n\")\n",
    "print(final_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
