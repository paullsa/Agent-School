{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de10884f",
   "metadata": {},
   "source": [
    "# OpenAI API to Ollama Gemma3:1b Code Conversion Prompting \n",
    "\n",
    "This code provides a Jupyter Notebook template that helps users convert legacy LangChain code into a modern version using local LLMs like `gemma3:1b` via Ollama. \n",
    "\n",
    "he first cell introduces the user to the purpose of the notebook: it helps transform old code that uses OpenAI models, `LLMChain`, and `SequentialChain` into the new LangChain architecture with `OllamaLLM` and `RunnableMap` for better flexibility and local execution. \n",
    "\n",
    "The second cell allows users to paste their legacy code, while the third cell generates a prompt that can be used with a local model or ChatGPT to convert the code automatically. It outputs the new, clean version using modern LangChain practices like `|` (piping) and `.invoke()`, along with comments to guide the user through the changes. \n",
    "\n",
    "This template streamlines the process of updating code for the latest LangChain patterns, making it easier for developers to maintain and scale their projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57168cea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "## üîÅ LangChain Code Converter\n",
       "\n",
       "Paste your legacy LangChain code into the next cell, then run the notebook to convert it to use:\n",
       "\n",
       "- ‚úÖ `langchain_ollama.OllamaLLM`\n",
       "- ‚úÖ Local model (`gemma3:1b` via Ollama)\n",
       "- ‚úÖ Modern LangChain Runnables (`|`, `.invoke()`, `RunnableMap`)\n",
       "- üö´ No more `LLMChain`, `SequentialChain`, or OpenAI API keys\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 1: Instructions\n",
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(\"\"\"\n",
    "## üîÅ LangChain Code Converter\n",
    "\n",
    "Paste your legacy LangChain code into the next cell, then run the notebook to convert it to use:\n",
    "\n",
    "- ‚úÖ `langchain_ollama.OllamaLLM`\n",
    "- ‚úÖ Local model (`gemma3:1b` via Ollama)\n",
    "- ‚úÖ Modern LangChain Runnables (`|`, `.invoke()`, `RunnableMap`)\n",
    "- üö´ No more `LLMChain`, `SequentialChain`, or OpenAI API keys\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e29942c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Paste your legacy code here as a raw string\n",
    "legacy_code = \"\"\"\n",
    "import os\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import openai\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm_model = \"gpt-3.5-turbo\"\n",
    "open_ai = OpenAI(temperature=0.7)\n",
    "\n",
    "template = \"Translate 'good morning' into {language}.\"\n",
    "prompt = PromptTemplate(input_variables=[\"language\"], template=template)\n",
    "\n",
    "chain = LLMChain(llm=open_ai, prompt=prompt)\n",
    "response = chain.run(language=\"French\")\n",
    "print(response)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51c55a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Paste this into ChatGPT (or a local LLM agent) to convert your code:\n",
      "\n",
      "\n",
      "Convert the following Python code from using OpenAI's `OpenAI`, `LLMChain`, or `SequentialChain` \n",
      "into the modern LangChain approach using:\n",
      "\n",
      "- The `langchain_ollama` package with the `OllamaLLM` class\n",
      "- A local Ollama model (use `gemma3:1b`)\n",
      "- The modern `Runnable` interfaces such as `prompt | llm` and `.invoke()`\n",
      "- No use of `.run()`, `LLMChain`, or `SequentialChain`\n",
      "- Remove `dotenv`, `openai`, or API key logic since Ollama runs locally\n",
      "\n",
      "Keep all the original logic and functionality intact.\n",
      "Add clear comments for educational purposes.\n",
      "Return only the updated code.\n",
      "\n",
      "Here is the code to convert:\n",
      "```python\n",
      "\n",
      "import os\n",
      "from dotenv import find_dotenv, load_dotenv\n",
      "import openai\n",
      "from langchain.chat_models import ChatOpenAI\n",
      "from langchain.llms import OpenAI\n",
      "from langchain.prompts import PromptTemplate\n",
      "from langchain.chains import LLMChain\n",
      "\n",
      "load_dotenv(find_dotenv())\n",
      "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
      "\n",
      "llm_model = \"gpt-3.5-turbo\"\n",
      "open_ai = OpenAI(temperature=0.7)\n",
      "\n",
      "template = \"Translate 'good morning' into {language}.\"\n",
      "prompt = PromptTemplate(input_variables=[\"language\"], template=template)\n",
      "\n",
      "chain = LLMChain(llm=open_ai, prompt=prompt)\n",
      "response = chain.run(language=\"French\")\n",
      "print(response)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Auto-convert the legacy code using a smart prompt (via local LLM or GPT)\n",
    "# If you have access to a local model (e.g. via LangChain agent or ChatGPT API), you'd do this programmatically.\n",
    "# Here we simulate it for the notebook.\n",
    "\n",
    "conversion_prompt = f\"\"\"\n",
    "Convert the following Python code from using OpenAI's `OpenAI`, `LLMChain`, or `SequentialChain` \n",
    "into the modern LangChain approach using:\n",
    "\n",
    "- The `langchain_ollama` package with the `OllamaLLM` class\n",
    "- A local Ollama model (use `gemma3:1b`)\n",
    "- The modern `Runnable` interfaces such as `prompt | llm` and `.invoke()`\n",
    "- No use of `.run()`, `LLMChain`, or `SequentialChain`\n",
    "- Remove `dotenv`, `openai`, or API key logic since Ollama runs locally\n",
    "\n",
    "Keep all the original logic and functionality intact.\n",
    "Add clear comments for educational purposes.\n",
    "Return only the updated code.\n",
    "\n",
    "Here is the code to convert:\n",
    "```python\n",
    "{legacy_code}\n",
    "\"\"\"\n",
    "print(\"üß† Paste this into ChatGPT (or a local LLM agent) to convert your code:\\n\")\n",
    "print(conversion_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b175cc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
