{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9398351",
   "metadata": {},
   "source": [
    "# Example of a Router Chain in LangChain\n",
    "\n",
    "https://python.langchain.com/docs/how_to/sequence/\n",
    "\n",
    "Again, this example is based off the great course The Complete LangChain LLMs Guide, but I've updated the code to reflect the newer LangChain runnables format, and also to show which template was used in the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18450590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”Ž Selected Expert: Biology\n",
      "ðŸ§  Answer:\n",
      "Alright, thatâ€™s a fantastic question! White blood cells, or leukocytes, are incredibly important in your body. Essentially, theyâ€™re your bodyâ€™s defense force. Hereâ€™s a breakdown of what they do:\n",
      "\n",
      "**Their main roles are:**\n",
      "\n",
      "*   **Immunity:** Theyâ€™re constantly patrolling your body, looking for signs of infection or disease.\n",
      "*   **Fighting Infections:** They release chemicals like histamine to fight bacteria and viruses.\n",
      "*   **Removing Waste:** They engulf and destroy cellular debris and damaged cells.\n",
      "*   **Repairing Tissue:** Some types help with wound healing.\n",
      "\n",
      "**In short, theyâ€™re your bodyâ€™s first line of defense against anything that doesnâ€™t belong!**\n",
      "\n",
      "Do you have any more specific questions about white blood cells, perhaps about their different types or how they function?\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Modern LangChain version using local Ollama model (gemma3:1b) and Runnable interfaces.\n",
    "\n",
    "This script utilizes LangChain to create a multi-prompt chain framework. It leverages \n",
    "the Ollama local model for NLP tasks, enabling users to pose questions across four domains: \n",
    "biology, mathematics, astronomy, and travel. A simple routing mechanism identifies the \n",
    "most relevant expert model to answer the query, ensuring domain-specific expertise is applied.\n",
    "\n",
    "Modules Used:\n",
    "- langchain_ollama: Interface to a local Ollama LLM model.\n",
    "- langchain_core.prompts: Provides tools to design structured, reusable prompt templates.\n",
    "- langchain_core.runnables: Builds executable chains of operations using LLMs and auxiliary functions.\n",
    "- langchain_core.output_parsers: Parses raw LLM output into user-friendly results.\n",
    "\"\"\"\n",
    "\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableMap, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Step 1: Initialize the local Ollama LLM\n",
    "# OllamaLLM provides access to the local model \"gemma3:1b\" to answer user questions.\n",
    "# The `temperature=0.0` ensures deterministic responses (the same input yields the same output every time).\n",
    "llm = OllamaLLM(model=\"gemma3:1b\", temperature=0.0)\n",
    "\n",
    "# Step 2: Define expert-specific prompt templates\n",
    "# Each expert prompt specifies the conversational tone, domain of expertise, and fallback behavior.\n",
    "# These templates guide the LLM in answering questions appropriately for each domain.\n",
    "\n",
    "biology_template = \"\"\"You are a very smart biology professor. \n",
    "You are great at answering questions about biology in a concise and easy-to-understand manner. \n",
    "When you don't know the answer to a question, you admit that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. You are great at answering math questions. \n",
    "You are so good because you are able to break down hard problems into their component parts, \n",
    "answer the component parts, and then put them together to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "astronomy_template = \"\"\"You are a very good astronomer. You are great at answering astronomy questions. \n",
    "You are so good because you are able to break down hard problems into their component parts, \n",
    "answer the component parts, and then put them together to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "travel_agent_template = \"\"\"You are a very good travel agent with a vast amount \n",
    "of knowledge when it comes to getting people the best deals and recommendations \n",
    "for travel, vacations, flights, and the world's best destinations for vacations. \n",
    "You are great at answering travel, vacation, flights, transportation, and tourist guides questions. \n",
    "You are so good because you are able to break down hard problems into their component parts, \n",
    "answer the component parts, and then put them together to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "# Step 3: Define metadata for expert models\n",
    "# A list is created to store the names, descriptions, and prompt templates for each domain.\n",
    "# This metadata will later be used to dynamically create chains for each expert.\n",
    "\n",
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"Biology\",\n",
    "        \"description\": \"Good for answering biology-related questions\",\n",
    "        \"prompt_template\": biology_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Math\",\n",
    "        \"description\": \"Good for answering math questions\",\n",
    "        \"prompt_template\": math_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Astronomy\",\n",
    "        \"description\": \"Good for answering astronomy questions\",\n",
    "        \"prompt_template\": astronomy_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Travel_Agent\",\n",
    "        \"description\": \"Good for answering travel, tourism, and vacation questions\",\n",
    "        \"prompt_template\": travel_agent_template,\n",
    "    },\n",
    "]\n",
    "\n",
    "# Step 4: Build domain-specific chains\n",
    "# This step creates a reusable chain for each expert, combining the prompt template, \n",
    "# LLM, and an output parser to structure responses. A dictionary maps each chain to its domain.\n",
    "\n",
    "destination_chains = {}\n",
    "for info in prompt_infos:\n",
    "    name = info[\"name\"]\n",
    "    template = ChatPromptTemplate.from_template(info[\"prompt_template\"])\n",
    "    chain = template | llm | StrOutputParser()  # Combines prompt, LLM, and parser into a single chain\n",
    "    destination_chains[name.lower()] = chain  # Normalize names to lowercase for consistency in routing\n",
    "\n",
    "# Step 5: Define a default chain for unmatched queries\n",
    "# When a query doesn't match any domain-specific keywords, this fallback chain handles it.\n",
    "# It uses a generic prompt template to structure the response.\n",
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "default_chain = default_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Step 6: Implement a router function\n",
    "# The router analyzes the input query for keywords and assigns it to the most relevant domain.\n",
    "# If no keywords match, the router assigns the query to the default chain.\n",
    "def simple_router(input: str) -> dict:\n",
    "    \"\"\"\n",
    "    Routes the input to the appropriate expert domain based on keywords in the query.\n",
    "    \n",
    "    Args:\n",
    "        input (str): User's query as a string.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing the determined destination domain and the original query.\n",
    "    \"\"\"\n",
    "    input_lower = input.lower()\n",
    "    if \"biology\" in input_lower or \"cell\" in input_lower or \"gene\" in input_lower:\n",
    "        destination = \"biology\"\n",
    "    elif \"math\" in input_lower or \"calculate\" in input_lower or \"equation\" in input_lower:\n",
    "        destination = \"math\"\n",
    "    elif \"star\" in input_lower or \"planet\" in input_lower or \"astronomy\" in input_lower:\n",
    "        destination = \"astronomy\"\n",
    "    elif \"vacation\" in input_lower or \"travel\" in input_lower or \"flight\" in input_lower:\n",
    "        destination = \"travel_agent\"\n",
    "    else:\n",
    "        destination = None  # Defaults to the fallback chain\n",
    "    return {\"destination\": destination, \"input\": input}\n",
    "\n",
    "# Step 7: Combine routing with chain execution\n",
    "# RunnableLambda applies the router function, and RunnableMap integrates chain execution logic.\n",
    "# The input query is dynamically routed to the appropriate expert chain based on keywords.\n",
    "router = RunnableLambda(simple_router)\n",
    "\n",
    "multi_prompt_chain = (\n",
    "    router |\n",
    "    RunnableMap({\n",
    "        # Structure the result: include the destination domain and LLM output.\n",
    "        \"destination\": lambda x: x[\"destination\"] or \"default\",  # Assign \"default\" if no domain matches\n",
    "        \"output\": lambda x: destination_chains.get(x[\"destination\"], default_chain).invoke(x[\"input\"])\n",
    "    })\n",
    ")\n",
    "\n",
    "# Step 8: Test the multi-prompt chain with a sample query\n",
    "if __name__ == \"__main__\":\n",
    "    user_question = \"What do white blood cells do?\"\n",
    "    result = multi_prompt_chain.invoke(user_question)\n",
    "    \n",
    "    # Display the determined expert domain and the generated response.\n",
    "    print(f\"\\nðŸ”Ž Selected Expert: {result['destination'].capitalize()}\")\n",
    "    print(\"ðŸ§  Answer:\")\n",
    "    print(result[\"output\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
