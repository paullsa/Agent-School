{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88f57d73",
   "metadata": {},
   "source": [
    "# üîç Document Question Answering with Local LLMs and Embeddings\n",
    "\n",
    "This notebook demonstrates how to build a **local, privacy-friendly document question answering (QA) system** using [LangChain](https://www.langchain.com/), [Ollama](https://ollama.com/), and [Chroma](https://www.trychroma.com/). It loads a PDF document, splits it into chunks, generates embeddings locally using the `nomic-embed-text` model, and stores them in a persistent Chroma vector database. A retriever fetches relevant chunks in response to a query, and a locally running LLM (e.g., `gemma3:1b`) is used to generate an accurate answer with citations. This workflow avoids cloud APIs, making it ideal for secure and offline environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60e5336e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Okay, let‚Äôs break down the two main challenges identified in the text regarding agentic workflows ‚Äì and then I‚Äôll expand on it.\n",
      "\n",
      "Based on the provided context, the two main challenges are:\n",
      "\n",
      "1.  **Ambiguity and Transient Interactions:** The text explicitly states that the interactions are ‚Äúbrief and transient.‚Äù This means that the collaboration between humans and AI agents is often short-lived.  The goal is to achieve complex goals, but the rapid, fragmented nature of these interactions makes it difficult to maintain a sustained engagement with users.  It‚Äôs hard to build a deep understanding of the user‚Äôs needs and intentions in a way that allows for continuous refinement.\n",
      "\n",
      "2.  **Limited Understanding of AI Functionalities:** The text highlights a lack of understanding of AI functionalities.  The AI agents are essentially acting as ‚Äústand-ins‚Äù for users, and the system isn‚Äôt yet equipped to fully grasp the nuances of how AI can truly assist in the design process.  This means the AI isn‚Äôt fully leveraging its capabilities to its full potential.\n",
      "\n",
      "**Expanding on the Challenges ‚Äì Why They Matter**\n",
      "\n",
      "These challenges are significant because they directly impact the effectiveness of agentic workflows.  Without a deeper understanding of *how* the AI is contributing and *what* it‚Äôs capable of, the workflow is inherently limited.  It‚Äôs like trying to build a complex machine without knowing how it works internally. \n",
      "\n",
      "Here‚Äôs a bit more detail about why these challenges are important:\n",
      "\n",
      "*   **Reduced User Engagement:**  Short, fleeting interactions are less likely to lead to deeper understanding and ultimately, more effective design.\n",
      "*   **Inefficient Design Process:**  The system isn‚Äôt optimizing the design process as effectively as it could.  It‚Äôs relying on quick, reactive actions rather than thoughtful, iterative design.\n",
      "*   **Missed Opportunities:** The AI‚Äôs potential to offer truly insightful guidance is being hampered by a lack of comprehension.\n",
      "\n",
      "**In short, the text suggests that the current approach needs to focus on building a system that can maintain a more sustained and nuanced engagement with users, and that requires a deeper understanding of the AI‚Äôs capabilities.**\n",
      "\n",
      "Do you want me to delve deeper into any of these challenges, or perhaps explore potential solutions or research directions that the text suggests?\n",
      "\n",
      "\n",
      "Sources:\n",
      "./data/Agentic_Workflows.pdf\n",
      "./data/Agentic_Workflows.pdf\n"
     ]
    }
   ],
   "source": [
    "# === Imports ===\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Import the necessary modules for working with documents, splitting text,\n",
    "# creating embeddings, managing vector storage, and performing question-answering.\n",
    "\n",
    "# === Load a PDF file ===\n",
    "loader = PyPDFLoader(\"./data/Agentic_Workflows.pdf\")  # Load the PDF file specified in the path\n",
    "docs = loader.load()  # Read the content of the PDF and prepare it for processing\n",
    "\n",
    "# === Split the document into manageable chunks ===\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # Set the maximum size of each chunk to 1000 characters\n",
    "    chunk_overlap=200  # Ensure 200 characters overlap between chunks for context\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)  # Split the loaded document into smaller chunks\n",
    "\n",
    "# === Use Ollama's nomic-embed-text to create embeddings ===\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text:latest\")  # Convert text into numerical representations (embeddings)\n",
    "\n",
    "# === Create a Chroma vectorstore with persistence ===\n",
    "persist_directory = './data/db/chroma/'  # Set the directory to save the vectorstore for later use\n",
    "\n",
    "# Store the embeddings and text chunks in a Chroma vectorstore, enabling persistent storage\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,  # The document chunks to store\n",
    "    embedding=embeddings,  # The embedding function to use for processing text\n",
    "    persist_directory=persist_directory  # Directory for storing data persistently\n",
    ")\n",
    "\n",
    "# === Load the persisted Chroma vectorstore ===\n",
    "vector_store = Chroma(\n",
    "    persist_directory=persist_directory,  # Load the saved vectorstore from the specified directory\n",
    "    embedding_function=embeddings  # Use the same embedding function as before\n",
    ")\n",
    "\n",
    "# === Set up a retriever from the vectorstore ===\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 2})  # Configure the retriever to return the top 2 most relevant results\n",
    "\n",
    "# === Use Ollama's local chat model ===\n",
    "llm = ChatOllama(model=\"gemma3:1b\", temperature=0.0)  # Initialize a chat model with deterministic responses (temperature set to 0.0)\n",
    "\n",
    "# === Set up a RetrievalQA chain for Q&A over documents ===\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,  # Use the language model for generating answers\n",
    "    chain_type=\"stuff\",  # Specify the type of chain to process retrieved document chunks\n",
    "    retriever=retriever,  # The retriever to fetch relevant document parts\n",
    "    verbose=True,  # Enable detailed logging for debugging\n",
    "    return_source_documents=True  # Include source documents in the response for transparency\n",
    ")\n",
    "\n",
    "# === Pretty-printing helper to show results and sources ===\n",
    "def process_llm_response(llm_response):\n",
    "    # Print the main result (answer) from the language model\n",
    "    print(llm_response['result'])\n",
    "    print('\\n\\nSources:')\n",
    "    # Print the sources used to generate the response\n",
    "    for source in llm_response[\"source_documents\"]:\n",
    "        print(source.metadata['source'])\n",
    "\n",
    "# === Ask a question! ===\n",
    "query = \"Talk to me about the two main challenges of agentic workflows\"  # Define a question to query the chain\n",
    "llm_response = qa_chain.invoke(query)  # Send the question to the RetrievalQA chain and get the response\n",
    "\n",
    "# === Display the result ===\n",
    "process_llm_response(llm_response)  # Format and print the answer along with its sources"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
