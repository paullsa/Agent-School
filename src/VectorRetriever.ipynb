{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88f57d73",
   "metadata": {},
   "source": [
    "# üîç Document Question Answering with Local LLMs and Embeddings\n",
    "\n",
    "This notebook demonstrates how to build a **local, privacy-friendly document question answering (QA) system** using [LangChain](https://www.langchain.com/), [Ollama](https://ollama.com/), and [Chroma](https://www.trychroma.com/). It loads a PDF document, splits it into chunks, generates embeddings locally using the `nomic-embed-text` model, and stores them in a persistent Chroma vector database. A retriever fetches relevant chunks in response to a query, and a locally running LLM (e.g., `gemma:2b`) is used to generate an accurate answer with citations. This workflow avoids cloud APIs, making it ideal for secure and offline environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60e5336e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Okay, let‚Äôs dive deeper into the ReAct prompting approach. Here‚Äôs a breakdown of what it is and why it‚Äôs significant:\n",
      "\n",
      "**What is ReAct?**\n",
      "\n",
      "ReAct is a prompting technique designed to improve language models‚Äô ability to solve complex tasks by combining reasoning and acting. It‚Äôs built around a core idea: **‚ÄúReasoning-Acting‚Äù**. Instead of just asking the model to *generate* an answer, ReAct encourages it to *first reason* about the problem, then *act* to try different solutions.\n",
      "\n",
      "**Here‚Äôs a breakdown of the key components:**\n",
      "\n",
      "1. **Reasoning Phase:** The model is given a task and a starting state. It then engages in a \"reasoning loop\" ‚Äì it generates a series of intermediate steps, thoughts, and hypotheses.  This is crucial because it moves beyond simply providing the answer and forces the model to *think* through the problem.\n",
      "\n",
      "2. **Acting Phase:** Based on the reasoning loop, the model *acts* ‚Äì it tries out different actions or strategies. It might generate a new state, modify its current state, or even ask for more information.\n",
      "\n",
      "3. **Looping:** This reasoning-acting cycle repeats multiple times. Each iteration refines the model's understanding and improves its ability to solve the task.\n",
      "\n",
      "**How it Works ‚Äì The Core Mechanism**\n",
      "\n",
      "The model is presented with a prompt like: \"Let's solve this problem. First, think about the possible actions you could take. Then, try one action and see what happens.  Then, think about the next possible action. Repeat this process until you find a solution.\"\n",
      "\n",
      "**Key Advantages & Why it's Important:**\n",
      "\n",
      "* **Few-Shot Learning:** ReAct excels at few-shot learning ‚Äì meaning it can learn to solve new tasks with just a few examples.  The reasoning loop helps the model quickly grasp the task's requirements.\n",
      "* **Improved Reasoning:** The explicit reasoning process forces the model to engage in more complex thought and problem-solving.\n",
      "* **Explainability:** The reasoning steps provide a way to understand *why* the model arrived at a particular solution, making it more transparent.\n",
      "* **Better Generalization:** By learning through a cycle of reasoning and acting, ReAct can generalize better to new, unseen tasks.\n",
      "\n",
      "**The Experiments & Results (as highlighted in the context):**\n",
      "\n",
      "* **Benchmark Performance:** The experiments demonstrate a significant improvement over prior approaches (like simply asking for the answer) in a few-shot learning setting.\n",
      "* **Focus on Acting:** The ablation studies highlight the importance of the \"acting\" component ‚Äì the model's ability to try different things is critical.\n",
      "* **Interactive Tasks:** The analysis shows that ReAct is particularly effective with interactive tasks, where the model needs to adapt its strategy based on feedback.\n",
      "\n",
      "\n",
      "**Resources to Learn More:**\n",
      "\n",
      "* **Original Paper:** [https://arxiv.org/abs/2303.16843](https://arxiv.org/abs/2303.16843) - This is the core research paper on ReAct.\n",
      "* **Blog Post:** [https://www.tensorflow.org/react](https://www.tensorflow.org/react) -  A good overview of the technique.\n",
      "\n",
      "---\n",
      "\n",
      "**Do you want me to:**\n",
      "\n",
      "*   Explain a specific aspect of ReAct in more detail (e.g., the reasoning loop)?\n",
      "*   Give you a more technical explanation of the prompting strategy?\n",
      "*   Compare ReAct to other prompting techniques?\n",
      "\n",
      "\n",
      "Sources:\n",
      "./data/react-paper.pdf\n",
      "./data/react-paper.pdf\n"
     ]
    }
   ],
   "source": [
    "# === Imports ===\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Import the necessary modules for working with documents, splitting text,\n",
    "# creating embeddings, managing vector storage, and performing question-answering.\n",
    "\n",
    "# === Load a PDF file ===\n",
    "loader = PyPDFLoader(\"./data/react-paper.pdf\")  # Load the PDF file specified in the path\n",
    "docs = loader.load()  # Read the content of the PDF and prepare it for processing\n",
    "\n",
    "# === Split the document into manageable chunks ===\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # Set the maximum size of each chunk to 1000 characters\n",
    "    chunk_overlap=200  # Ensure 200 characters overlap between chunks for context\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)  # Split the loaded document into smaller chunks\n",
    "\n",
    "# === Use Ollama's nomic-embed-text to create embeddings ===\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text:latest\")  # Convert text into numerical representations (embeddings)\n",
    "\n",
    "# === Create a Chroma vectorstore with persistence ===\n",
    "persist_directory = './data/db/chroma/'  # Set the directory to save the vectorstore for later use\n",
    "\n",
    "# Store the embeddings and text chunks in a Chroma vectorstore, enabling persistent storage\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,  # The document chunks to store\n",
    "    embedding=embeddings,  # The embedding function to use for processing text\n",
    "    persist_directory=persist_directory  # Directory for storing data persistently\n",
    ")\n",
    "\n",
    "# === Load the persisted Chroma vectorstore ===\n",
    "vector_store = Chroma(\n",
    "    persist_directory=persist_directory,  # Load the saved vectorstore from the specified directory\n",
    "    embedding_function=embeddings  # Use the same embedding function as before\n",
    ")\n",
    "\n",
    "# === Set up a retriever from the vectorstore ===\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 2})  # Configure the retriever to return the top 2 most relevant results\n",
    "\n",
    "# === Use Ollama's local chat model ===\n",
    "llm = ChatOllama(model=\"gemma3:1b\", temperature=0.0)  # Initialize a chat model with deterministic responses (temperature set to 0.0)\n",
    "\n",
    "# === Set up a RetrievalQA chain for Q&A over documents ===\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,  # Use the language model for generating answers\n",
    "    chain_type=\"stuff\",  # Specify the type of chain to process retrieved document chunks\n",
    "    retriever=retriever,  # The retriever to fetch relevant document parts\n",
    "    verbose=True,  # Enable detailed logging for debugging\n",
    "    return_source_documents=True  # Include source documents in the response for transparency\n",
    ")\n",
    "\n",
    "# === Pretty-printing helper to show results and sources ===\n",
    "def process_llm_response(llm_response):\n",
    "    # Print the main result (answer) from the language model\n",
    "    print(llm_response['result'])\n",
    "    print('\\n\\nSources:')\n",
    "    # Print the sources used to generate the response\n",
    "    for source in llm_response[\"source_documents\"]:\n",
    "        print(source.metadata['source'])\n",
    "\n",
    "# === Ask a question! ===\n",
    "query = \"tell me more about ReAct prompting\"  # Define a question to query the chain\n",
    "llm_response = qa_chain.invoke(query)  # Send the question to the RetrievalQA chain and get the response\n",
    "\n",
    "# === Display the result ===\n",
    "process_llm_response(llm_response)  # Format and print the answer along with its sources"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
