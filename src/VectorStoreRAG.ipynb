{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1888ab06",
   "metadata": {},
   "source": [
    "# Vector Store RAge - A Local AI-Powered Search System\n",
    "\n",
    "The code builds a local AI-powered search system. It lets you ask questions about a collection of documents, and the AI will respond using only those documents to answer you.\n",
    "\n",
    "You get a reliable, document-grounded answer to your question ‚Äî all done locally using your own files and models.\n",
    "\n",
    "The user_query field is where you enter the particular question you have\n",
    "\n",
    "The final output you get can vary somewhat, likely caused by the small size of the models since you have to run them locally "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8407d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "\n",
    "# Installs the `unstructured` package, which provides tools for parsing and extracting data from unstructured documents such as PDFs, text files, and images.\n",
    "!pip install unstructured\n",
    "\n",
    "# Installs `faiss-cpu`, a CPU-only version of FAISS (Facebook AI Similarity Search). FAISS is used for efficient similarity search and clustering of high-dimensional vectors, commonly used for tasks like information retrieval and recommendation systems.\n",
    "!pip install faiss-cpu\n",
    "\n",
    "# Installs `langchain`, an open-source framework for building applications that integrate with language models (such as GPT). It provides a unified interface for working with AI models, document loaders, and other tools in your workflows.\n",
    "!pip install langchain\n",
    "\n",
    "# Installs the `langchain_community` package, which contains additional community-contributed components for LangChain, including custom document loaders, retrievers, and other enhancements that are not part of the core LangChain library.\n",
    "!pip install langchain_community\n",
    "\n",
    "# Installs `langchain_ollama`, a package that enables integration with Ollama‚Äôs local models for language model-based tasks. It allows you to run models like `phi-4-mini` locally without the need for an API call.\n",
    "!pip install langchain_ollama\n",
    "\n",
    "# Installs `python-magic-bin`, a precompiled binary version of `python-magic` for Windows. `python-magic` is a Python interface to the `libmagic` file type detection library, which helps automatically detect file types based on file content. This is useful when working with mixed document types (e.g., PDFs, text files) in tasks like document loading.\n",
    "!pip install python-magic-bin\n",
    "\n",
    "# Installs `PyMuPDF`, a Python binding for MuPDF, a lightweight PDF and XPS viewer. It provides tools for working with PDF files, including text extraction, rendering, and manipulation. This is useful for parsing and extracting data from PDF documents in the context of unstructured data processing.\n",
    "!pip install pymupdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634a643a",
   "metadata": {},
   "source": [
    "### Part 1: Prepare the documents and setup the vectorstore\n",
    "\n",
    "Load files from a folder by reading files from a local folder on your computer.\n",
    "\n",
    "Break them into smaller pieces - since documents can be long, chop them into smaller, overlapping chunks for easier processing.\n",
    "\n",
    "Convert text into numbers (embeddings) - use a local model (via Ollama) to turn each chunk into a special set of numbers that represent the meaning of the text. This is called an embedding.\n",
    "\n",
    "Store these embeddings in a searchable index - save everything into a database (called a vector store, using FAISS) so it can later find similar chunks when you ask a question.\n",
    "\n",
    "Save it for later use - the index is saved to your computer so you don‚Äôt have to rebuild it each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "353895bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Loaded 51 documents.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "\n",
    "# Define a custom loader for mixed file types (txt, csv, pdf)\n",
    "# This custom loader class allows us to handle different types of document formats in a single unified way.\n",
    "class CustomDocumentLoader:\n",
    "    def __init__(self, path, glob=\"**/*.{txt,csv,pdf}\"):\n",
    "        # `path`: The directory path where the files are located.\n",
    "        # `glob`: A glob pattern that specifies which file types to look for, in this case .txt, .csv, and .pdf.\n",
    "        self.path = path\n",
    "        self.glob = glob\n",
    "    \n",
    "    def load(self):\n",
    "        # Manually load files from the directory using the glob pattern.\n",
    "        # We initialize an empty list `documents` to store the loaded content.\n",
    "        documents = []\n",
    "        \n",
    "        # Recursively traverse the directory and check each file's extension.\n",
    "        # The `os.walk()` function iterates over all directories and files within `self.path`.\n",
    "        for root, _, files in os.walk(self.path):\n",
    "            for file in files:\n",
    "                # Construct the full file path.\n",
    "                file_path = os.path.join(root, file)\n",
    "                \n",
    "                # Check file extension to determine how to load the file.\n",
    "                if file.endswith('.txt'):\n",
    "                    # If it's a text file, open it and read its content.\n",
    "                    with open(file_path, 'r') as f:\n",
    "                        # Append the content of the text file to the `documents` list.\n",
    "                        documents.append(f.read())\n",
    "                elif file.endswith('.csv'):\n",
    "                    # If it's a CSV file, use the CSVLoader to load the CSV content.\n",
    "                    # CSVLoader will read the CSV and split it into rows or columns as needed.\n",
    "                    csv_loader = CSVLoader(file_path=file_path)\n",
    "                    # Extend the documents list with the content from the CSV loader.\n",
    "                    documents.extend(csv_loader.load())\n",
    "                elif file.endswith('.pdf'):\n",
    "                    # If it's a PDF file, use PyMuPDFLoader to extract text from the PDF.\n",
    "                    # PyMuPDFLoader reads the PDF content, extracting text from each page.\n",
    "                    pdf_loader = PyMuPDFLoader(file_path=file_path)\n",
    "                    # Extend the documents list with the content from the PDF loader.\n",
    "                    documents.extend(pdf_loader.load())\n",
    "        \n",
    "        # Return the list of loaded documents (text from .txt, .csv, and .pdf files).\n",
    "        return documents\n",
    "\n",
    "# Use the CustomDocumentLoader to load .txt, .csv, and .pdf files\n",
    "# Specify the directory where the documents are stored.\n",
    "loader = CustomDocumentLoader(r\"C:\\Python\\Agent-School\\docs\\docs2\")\n",
    "\n",
    "# Load the documents using the custom loader. This will load all .txt, .csv, and .pdf files from the specified directory.\n",
    "docs = loader.load()\n",
    "\n",
    "# Now `docs` contains all the documents (text, CSV, and PDF content) that were loaded.\n",
    "# Print the number of documents that were loaded to give feedback on how many files were processed.\n",
    "print(f\"üìÑ Loaded {len(docs)} documents.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c46c2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Vector store created and saved.\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 1. Load local text/PDF/Markdown/etc. files\n",
    "# Use the DirectoryLoader to load documents from a specified folder.\n",
    "# In this case, we are loading text files (with `.txt` extension) from the directory path \"C:\\Python\\Agent-School\\docs\\docs2\".\n",
    "# The `glob=\"**/*.txt\"` pattern ensures that the loader will pick up all `.txt` files in the directory and its subdirectories.\n",
    "loader = DirectoryLoader(r\"C:\\Python\\Agent-School\\docs\\docs2\", glob=\"**/*.txt\")\n",
    "\n",
    "# Load the documents into the `docs` variable. This will return a list of documents, \n",
    "# each of which will contain the text content of the corresponding file.\n",
    "docs = loader.load()\n",
    "\n",
    "# 2. Split into manageable chunks\n",
    "# We use the `RecursiveCharacterTextSplitter` to split the loaded documents into smaller chunks.\n",
    "# The `chunk_size=500` argument specifies that each chunk should contain at most 500 characters.\n",
    "# The `chunk_overlap=50` argument ensures that there will be an overlap of 50 characters between consecutive chunks, \n",
    "# which can help preserve context when processing the document.\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "# Split the documents into chunks and store the result in `chunks`.\n",
    "# Each element in `chunks` will be a smaller piece of text from the original documents.\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "# 3. Create embeddings using Ollama's local model\n",
    "# Now we create an embedding model using Ollama's local \"nomic-embed-text\" model.\n",
    "# Embeddings are vector representations of text that capture semantic meaning, which are used for similarity searches.\n",
    "embedding_model = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "# 4. Create FAISS index\n",
    "# FAISS is used to create an index for efficient similarity search. We use the `from_documents` method to generate embeddings for each document chunk.\n",
    "# The `embedding_model` generates embeddings for the chunks, which are then stored in the FAISS vector store.\n",
    "vectorstore = FAISS.from_documents(chunks, embedding_model)\n",
    "\n",
    "# 5. Save locally\n",
    "# Save the created FAISS vector store to the local disk so it can be reused in the future.\n",
    "# The vector store will be saved in the directory \"faiss_index_docs2\".\n",
    "vectorstore.save_local(\"faiss_index_docs2\")\n",
    "\n",
    "# Print confirmation that the vector store has been created and saved successfully.\n",
    "print(\"‚úÖ Vector store created and saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f151dc",
   "metadata": {},
   "source": [
    "### Part 2: Ask a question and get an answer (rag_phi4_query)\n",
    "Load the saved vector store i.e. load the document database created earlier.\n",
    "\n",
    "Find the most relevant chunks - when you ask a question (like \"Describe the Apollo programme\"), it searches the index and pulls out the top 5 chunks most related to your question.\n",
    "\n",
    "Create a grounded prompt - it puts those chunks into a prompt that tells the AI:\n",
    "‚ÄúOnly use this information to answer ‚Äî don‚Äôt make things up.‚Äù\n",
    "\n",
    "Ask a local language model (phi4-mini) - it sends the prompt to a small AI model running locally (no internet needed), and the model generates a thoughtful answer based only on the context it was given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38707475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Answer:\n",
      "The Apollo program was an American initiative aimed at landing humans on the Moon as part of President John F. Kennedy's 1961 goal for human exploration beyond Earth orbit (HEO). Announced by JFK, its ambitious objective culminated with astronauts Neil Armstrong and Buzz Aldrin stepping onto lunar soil during NASA‚Äôs historic mission in July-August 1969.\n",
      "\n",
      "Subsequent Apollo missions expanded upon the initial success of landing humans on a celestial body. They conducted extended trips to explore Earth-orbiting satellites while also collecting samples from various locations both around our planet as well as others throughout interstellar space that were previously unreachable by man, with many being returned for study and analysis back here at home.\n",
      "\n",
      "The Apollo program had three significant goals: 1) demonstrate human capability in achieving HEO; 2) test new spacecraft technologies (particularly the Saturn V rocket which carried humans beyond Earth‚Äôs atmosphere); & 3) retrieve samples of lunar soil that would help scientists understand more about our neighbor Moon. Ultimately, these accomplishments helped mankind better grasp what it meant to travel into space and laid a foundation for future exploration endeavors.\n",
      "\n",
      "Overall, while there were some failures along this journey (such as Apollo I's fire in January ‚Äô68), most things went according plan by NASA engineers & astronauts alike which showed how far humanity has come through collaboration with private partners such as Blue Origin & Virgin Galactic who continue pushing boundaries even further today. Thanks to all those dedicated professionals working together toward an end goal that once seemed impossible! With each passing year, new technological advancements are being developed while simultaneously building upon existing knowledge gained from past successes; this allows us the luxury of looking back at what we achieved before jumping ahead into unknown territory as though it were no big deal whatsoever- which thankfully isn‚Äôt true!\n",
      "\n",
      "The Moon Landings\n",
      "\n",
      "Exploring The Final Frontier: United States Developments in Space Exploration. \n",
      "\n",
      "In conclusion, space exploration has come a long way since its beginnings with NASA's Apollo missions paving the path towards achieving unprecedented goals including sending humans beyond our solar system as well here at home through collaboration between private companies like Blue Origin & Virgin Galactic who continue pushing boundaries even further today by developing new technologies which allows them to build upon existing knowledge gained from past successes; this shows how far humanity has come when working together on projects that once seemed impossible. With continued dedication towards exploring unknown territories beyond Earth, there's no limit what could possibly be achieved next!\n"
     ]
    }
   ],
   "source": [
    "# Perform RAG query\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "import ollama\n",
    "\n",
    "# 1. Load FAISS vector store\n",
    "# Create an embedding model using Ollama's \"nomic-embed-text\" model.\n",
    "# This model will be used to convert documents and queries into embeddings (vector representations).\n",
    "embedding_model = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "# Load a pre-trained FAISS vector store from the local disk.\n",
    "# The vector store is assumed to be located in the folder \"faiss_index_docs2\".\n",
    "# The `embedding_model` ensures that the embeddings are processed in the right way when loading the vector store.\n",
    "# `allow_dangerous_deserialization=True` is used to allow loading potentially unsafe data, so only use it with trusted sources.\n",
    "vectorstore = FAISS.load_local(\n",
    "    \"faiss_index_docs2\",\n",
    "    embedding_model,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\n",
    "# 2. Retrieve top-k relevant chunks\n",
    "# Create a retriever from the vector store that will search for the most similar document chunks.\n",
    "# The retriever is configured to return the top 5 most relevant chunks (`k=5`).\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# Define the user query (question) we want to ask the system.\n",
    "user_query = \"Describe the Apollo programme\"\n",
    "\n",
    "# Use the retriever to search for the top 5 most relevant documents from the vector store based on the user query.\n",
    "# The result is stored in the `docs` variable, which contains the retrieved document chunks.\n",
    "docs = retriever.invoke(user_query)\n",
    "\n",
    "# 3. Construct grounded prompt\n",
    "# Combine the content of the retrieved documents into a single string, with each document separated by a double newline (`\\n\\n`).\n",
    "# This string will serve as the context for the language model.\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "# Create the final prompt to send to the language model.\n",
    "# The prompt includes instructions for the model to use only the information from the context to answer the question.\n",
    "# If the answer cannot be found in the context, the model is instructed to say \"I don't have enough information to answer that.\"\n",
    "prompt = f\"\"\"\n",
    "You are a helpful assistant.\n",
    "\n",
    "Use only the information in the context below to answer the question.\n",
    "If the answer is not in the context, say:\n",
    "\"I don't have enough information to answer that.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {user_query}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "# 4. Ask phi-4-mini via Ollama\n",
    "# Send the constructed prompt to the phi-4-mini model using the Ollama API.\n",
    "# The `messages` parameter contains the role of the sender (in this case, \"user\") and the content of the message (the prompt).\n",
    "# The response from the model will be stored in the `response` variable.\n",
    "response = ollama.chat(\n",
    "    model=\"phi4-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    ")\n",
    "\n",
    "# 5. Output the Answer\n",
    "# Extract and print the answer from the model's response.\n",
    "# The answer is located in the `message[\"content\"]` field of the response.\n",
    "print(\"\\nüß† Answer:\")\n",
    "print(response[\"message\"][\"content\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
