{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c566b40",
   "metadata": {},
   "source": [
    "# This is some really simple code which illustrates how to involve a chain in LangChain\n",
    "\n",
    "This code uses a locally hosted AI language model (Gemma 3B) through  Ollama to translate the phrase \"Good morning\" into another language. It defines a prompt with a placeholder for the language, then fills in that placeholder with a specific language (in this case, German) and sends the prompt to the AI model. The model processes the prompt and returns a response with the translation. The code uses LangChain's newer, streamlined way of combining components (called \"runnables\") to make the process flexible and easy to extend later. Finally, it prints the translated phrase to the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e67aed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a few ways to say \"good morning\" in German, depending on the formality and dialect:\n",
      "\n",
      "* **Guten Morgen:** This is the most common and generally accepted way to say \"good morning.\" It’s suitable for most situations.\n",
      "\n",
      "* **Morgen:** This is a shorter, more casual way to say \"morning.\" It’s perfectly acceptable in informal settings.\n",
      "\n",
      "So, to summarize: **Guten Morgen** is the best choice for most situations.\n",
      "\n",
      "Do you want to know more about the nuances of German greetings?\n"
     ]
    }
   ],
   "source": [
    "# Import the OllamaLLM wrapper from the updated `langchain_ollama` package.\n",
    "# This is the recommended way to use locally-hosted LLMs like Gemma through Ollama.\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "# Import the PromptTemplate class for defining dynamic prompts.\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Initialize the local LLM using the `gemma3:1b` model.\n",
    "# - `model=\"gemma3:1b\"` tells Ollama which model to use (make sure it’s pulled with `ollama pull gemma3:1b`)\n",
    "# - `temperature=0.7` adds a bit of randomness to the output (0 is deterministic, 1 is highly creative)\n",
    "llm = OllamaLLM(model=\"gemma3:1b\", temperature=0.7)\n",
    "\n",
    "# Create a prompt template for the language task.\n",
    "# - `input_variables=[\"language\"]` defines a placeholder that we’ll fill dynamically.\n",
    "# - `template=...` defines the actual prompt text, where `{language}` will be substituted at runtime.\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"language\"],\n",
    "    template=\"How do you say good morning in {language}?\"\n",
    ")\n",
    "\n",
    "# Combine the prompt and LLM into a pipeline using LangChain’s Runnable interface.\n",
    "# - This is the modern, composable approach in LangChain (using `|` to pipe components).\n",
    "# - The result is a chain that accepts an input dict and returns the LLM's output.\n",
    "chain = prompt | llm\n",
    "\n",
    "# Execute the chain by providing a dictionary with the required input variable.\n",
    "# - Here, we're asking how to say \"good morning\" in German.\n",
    "# - `invoke()` is the preferred method to run chains or tools in the modern API.\n",
    "response = chain.invoke({\"language\": \"German\"})\n",
    "\n",
    "# Print the final output returned by the LLM.\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
