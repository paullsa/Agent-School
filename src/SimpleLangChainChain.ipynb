{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c566b40",
   "metadata": {},
   "source": [
    "# This is some really simple code which illustrates how to involve a chain in LangChain\n",
    "\n",
    "This code uses a locally hosted AI language model (Gemma 3B) through  Ollama to translate the phrase \"Good morning\" into another language. It defines a prompt with a placeholder for the language, then fills in that placeholder with a specific language (in this case, German) and sends the prompt to the AI model. The model processes the prompt and returns a response with the translation. The code uses LangChain's newer, streamlined way of combining components (called \"runnables\") to make the process flexible and easy to extend later. Finally, it prints the translated phrase to the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e67aed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a few ways to say \"good morning\" in French, depending on the level of formality:\n",
      "\n",
      "*   **Bonjour:** This is the most common and versatile way to say \"good morning.\" It's appropriate for almost any situation.\n",
      "\n",
      "*   **Bon matin:** This literally means \"good morning,\" and is perfectly acceptable, but less common than ‚ÄúBonjour.‚Äù\n",
      "\n",
      "So, you would say **Bonjour**! \n",
      "\n",
      "üòä\n"
     ]
    }
   ],
   "source": [
    "# Import the OllamaLLM wrapper from the updated `langchain_ollama` package.\n",
    "# This is the recommended way to use locally-hosted LLMs like Gemma through Ollama.\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "# Import the PromptTemplate class for defining dynamic prompts.\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Initialize the local LLM using the `gemma3:1b` model.\n",
    "# - `model=\"gemma3:1b\"` tells Ollama which model to use (make sure it‚Äôs pulled with `ollama pull gemma3:1b`)\n",
    "# - `temperature=0.7` adds a bit of randomness to the output (0 is deterministic, 1 is highly creative)\n",
    "llm = OllamaLLM(model=\"gemma3:1b\", temperature=0.7)\n",
    "\n",
    "# Create a prompt template for the language task.\n",
    "# - `input_variables=[\"language\"]` defines a placeholder that we‚Äôll fill dynamically.\n",
    "# - `template=...` defines the actual prompt text, where `{language}` will be substituted at runtime.\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"language\"],\n",
    "    template=\"How do you say good morning in {language}?\"\n",
    ")\n",
    "\n",
    "# Combine the prompt and LLM into a pipeline using LangChain‚Äôs Runnable interface.\n",
    "# - This is the modern, composable approach in LangChain (using `|` to pipe components).\n",
    "# - The result is a chain that accepts an input dict and returns the LLM's output.\n",
    "chain = prompt | llm\n",
    "\n",
    "# Execute the chain by providing a dictionary with the required input variable.\n",
    "# - Here, we're asking how to say \"good morning\" in German.\n",
    "# - `invoke()` is the preferred method to run chains or tools in the modern API.\n",
    "response = chain.invoke({\"language\": \"French\"})\n",
    "\n",
    "# Print the final output returned by the LLM.\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ac2abd",
   "metadata": {},
   "source": [
    "You can now build on this to have more inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d38d4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here‚Äôs a short lullaby inspired by Scotland and Isla, aiming for a gentle and calming feel:\n",
      "\n",
      "(Soft, gentle melody ‚Äì think acoustic guitar)\n",
      "\n",
      "Sleepy Isla, sun so bright,\n",
      "Across the hills, a golden light.\n",
      "Scotland whispers, soft and low,\n",
      "Where heather blooms and rivers flow.\n",
      "\n",
      "The mountains sleep, a misty hue,\n",
      "And little sheep graze, fresh and true.\n",
      "The stars are twinkling, clear and bold,\n",
      "A story whispered, to be told.\n",
      "\n",
      "Sleep now, Isla, safe and deep,\n",
      "While Scotland‚Äôs secrets softly sleep. \n",
      "Goodnight, my love, sleep with me. \n",
      "\n",
      "---\n",
      "\n",
      "**Notes:**\n",
      "\n",
      "*   **Imagery:** I‚Äôve focused on visuals of Scotland ‚Äì hills, heather, rivers, and sheep.\n",
      "*   **Tone:** Gentle and soothing.\n",
      "*   **Length:**  Approximately 100 words.\n",
      "\n",
      "Would you like me to revise it, perhaps with a specific focus (e.g., a particular landscape)?\n"
     ]
    }
   ],
   "source": [
    "# No need for OpenAI keys or dotenv when using Ollama locally\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Load the Gemma 3B model locally via Ollama\n",
    "# Make sure to run `ollama pull gemma3:1b` before using this\n",
    "llm = OllamaLLM(model=\"gemma3:1b\", temperature=0.7)\n",
    "\n",
    "# Define the prompt template for generating a children's lullaby\n",
    "template = \"\"\" \n",
    "As a children's poem writer, please come up with a simple and short (100 word)\n",
    "lullaby based on the location\n",
    "{location}\n",
    "and the main character {name}\n",
    "\n",
    "STORY:\n",
    "\"\"\"\n",
    "\n",
    "# Create a dynamic prompt using placeholders for location and name\n",
    "prompt = PromptTemplate(input_variables=[\"location\", \"name\"], template=template)\n",
    "\n",
    "# Create the processing chain using the modern Runnable interface\n",
    "chain = prompt | llm\n",
    "\n",
    "# Invoke the chain with specific inputs\n",
    "response = chain.invoke({\"location\": \"Scotland\", \"name\": \"Isla\"})\n",
    "\n",
    "# Print the output text from the model\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
