{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93e79f3c",
   "metadata": {},
   "source": [
    "### LangChain Vectorstore RAG Implementation\n",
    "---\n",
    "This notebook demonstrates a Retrieval-Augmented Generation (RAG) system using LangChain with local models via Ollama. The implementation follows a multi-step reasoning process:\n",
    "\n",
    "1. **Setup**: Loads two Ollama models (phi4-mini for reasoning and Gemma3:1b for synthesis) to handle different parts of the process.\n",
    "\n",
    "2. **Question Analysis**: Uses the reasoning model to break down complex questions into logical sub-steps that can be individually researched.\n",
    "\n",
    "3. **Document Processing**: Loads a local text corpus about space exploration, splits it into manageable chunks, and creates vector embeddings using the nomic-embed-text model.\n",
    "\n",
    "4. **Knowledge Retrieval**: For each identified reasoning step, performs a similarity search in the Chroma vectorstore to find the most relevant information from the knowledge base.\n",
    "\n",
    "5. **Answer Synthesis**: Feeds the original question and all retrieved contextual information to the synthesis model, which generates a cohesive, factual response.\n",
    "\n",
    "This approach enhances the quality of AI-generated answers by combining structured reasoning with targeted information retrieval from a domain-specific knowledge base, allowing for more accurate and contextually relevant responses than using an LLM alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8655eb44",
   "metadata": {},
   "source": [
    "#### To Do List\n",
    "\n",
    "Add multiple documents to the doc store with the information needed to answer the question split between different documents\n",
    "Add a PDF or other file type to make the search deal with different types of source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7516613b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required libraries from requirements.txt \n",
    "# pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49fbf006",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Import the built-in regular expressions module for pattern matching and text processing\n",
    "    import re\n",
    "\n",
    "    # Import the Ollama LLM class from the LangChain community package (often used for integrating local LLMs)\n",
    "    from langchain_community.llms import Ollama\n",
    "\n",
    "    # Import the PromptTemplate class used to define and structure prompts for LLMs\n",
    "    from langchain.prompts import PromptTemplate\n",
    "\n",
    "    # Import RunnableMap, a utility for composing and executing a sequence of runnable components\n",
    "    from langchain_core.runnables import RunnableMap\n",
    "\n",
    "    # Import Chroma vector store, used for storing and searching vector embeddings (RAG retrieval)\n",
    "    from langchain.vectorstores import Chroma\n",
    "\n",
    "    # Import Ollama-specific embeddings and LLM classes for use with LangChain\n",
    "    from langchain_ollama import OllamaEmbeddings, OllamaLLM\n",
    "\n",
    "    # Import TextLoader to load plain text documents from files for processing\n",
    "    from langchain.document_loaders import TextLoader\n",
    "\n",
    "    # Import CharacterTextSplitter to split large documents into smaller chunks based on character count\n",
    "    from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"Import error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error during imports: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9f32680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Ensure Ollama models are available ---\n",
    "# Attempt to load the reasoning and synthesis LLM models from Ollama.\n",
    "# If the models are not available, provide instructions to the user and exit.\n",
    "try:\n",
    "    reasoning_llm = OllamaLLM(model=\"phi4-mini\")\n",
    "    synthesis_llm = OllamaLLM(model=\"Gemma3:1b\")\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Failed to connect to Ollama or load model 'phi4-mini'.\")\n",
    "    print(\"üí° Make sure Ollama is running and the model is available:\")\n",
    "    print(\"    ollama run phi4-mini\")\n",
    "    print(f\"Error details: {e}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fe6aa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Prompt to break down the question ---\n",
    "# Define a prompt template to break down a question into logical steps.\n",
    "# This uses the reasoning LLM to generate a step-by-step breakdown.\n",
    "reasoning_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a reasoning assistant. Break the following question into logical steps to help answer it:\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Step-by-step breakdown:\n",
    "\"\"\")\n",
    "step_chain = reasoning_prompt | reasoning_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2c09b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 505, which is longer than the specified 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# --- Step 3: Set up vectorstore with Chroma ---\n",
    "# Load a local text corpus using TextLoader.\n",
    "loader = TextLoader(\"C:\\Python\\Agent-School\\docs\\Space.txt\")  # Load your local corpus\n",
    "documents = loader.load()\n",
    "\n",
    "# Split the documents into smaller chunks for better processing.\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# Set up embeddings using the Ollama 'nomic-embed-text' model.\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "# Create a vectorstore (Chroma) using the split documents and embeddings.\n",
    "vectorstore = Chroma.from_documents(split_docs, embeddings)\n",
    "\n",
    "# Here's example of how to persist the vectorstore to disk:\n",
    "# You can install the SQLite VSCode extension to view the database.\n",
    "# Use ctrl+shift+p to open the command palette and type \"SQLite\" to find the extension.\n",
    "# Then choose SQLIte: Open Database and select the database file.\n",
    "persist_directory = \"./data/db/chroma\"\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=split_docs,\n",
    "    embedding=embeddings, # openai embeddings\n",
    "    persist_directory=persist_directory)\n",
    "\n",
    "print(vectorstore._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "717f07cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Prompt for final synthesis ---\n",
    "# Define a prompt template for synthesizing a final answer.\n",
    "# This uses the synthesis LLM to generate a complete and informative response.\n",
    "# Remember - this is just a template, with {question} and {facts} as placeholders which are populated later.\n",
    "\n",
    "synthesis_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Based on the following question and information, write a complete, informative answer.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Information:\n",
    "{facts}\n",
    "\n",
    "Answer:\n",
    "\"\"\")\n",
    "synthesis_chain = synthesis_prompt | synthesis_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d9489b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Reasoning steps:\n",
      "\n",
      "Sure! Let's break down this task step by step.\n",
      "\n",
      "1. Identify which event is being referred to as \"the moon landing.\"\n",
      "   - The Moon Landing refers specifically to Apollo 11's mission where Neil Armstrong first walked on the lunar surface in July 1969.\n",
      "  \n",
      "2. Determine who was President of the United States at that time (July-August 1969).\n",
      "   - Richard Nixon served as U.S. President from January 20, 1969.\n",
      "\n",
      "3. Find out what policies or initiatives related to space exploration were implemented by this president during his tenure around August-September 1969.\n",
      "   - Although there isn't a well-documented specific \"policy\" directly linked with the Apollo 11 mission itself (it was primarily executed under President John F. Kennedy's administration), Richard Nixon continued many aspects of previous administrations' policies on space exploration, including NASA‚Äôs projects.\n",
      "\n",
      "4. Summarize this information:\n",
      "    - The U.S. president during the moon landing in July-August 1969 is identified as Richard Nixon.\n",
      "    - His policy towards ongoing efforts related to Apollo missions and general continuation was an extension or support of existing initiatives rather than introducing new groundbreaking policies, continuing with NASA‚Äôs projects initiated by his predecessors.\n",
      "\n",
      "Final Answer:\n",
      "- The U.S. president during the moon landing (Apollo 11) in July-August 1969 is Richard Nixon.\n",
      "- His policy on space exploration involved supporting and extending ongoing efforts primarily focused through established programs under prior administrations rather than introducing new specific policies for Apollo missions directly at that time, continuing NASA's projects initiated by previous presidents such as John F. Kennedy.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Step 5: Ask a question ---\n",
    "# Define the question to be answered and invoke the reasoning chain to get step-by-step reasoning.\n",
    "question = \"Who was the U.S. president during the moon landing, and what was his policy on space exploration?\"\n",
    "steps_text = step_chain.invoke({\"question\": question})\n",
    "\n",
    "# Print the reasoning steps generated by the LLM.\n",
    "print(\"\\nüß† Reasoning steps:\\n\")\n",
    "print(steps_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de3c6ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 6: Parse reasoning steps ---\n",
    "# Extract individual reasoning steps from the generated text using regex.\n",
    "step_lines = re.findall(r\"\\d+\\.\\s+(.*)\", steps_text)\n",
    "facts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf0f1ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Looking up: Identify which event is being referred to as \"the moon landing.\"\n",
      "\n",
      "üîç Looking up: 2. Determine who was President of the United States at that time (July-August 1969).\n",
      "\n",
      "üîç Looking up: 3. Find out what policies or initiatives related to space exploration were implemented by this president during his tenure around August-September 1969.\n",
      "\n",
      "üîç Looking up: Summarize this information:\n",
      "\n",
      "üìö Retrieved facts from Vectorstore:\n",
      "\n",
      "- Identify which event is being referred to as \"the moon landing.\": ### Nixon's Role in the Moon Landing\n",
      "When Nixon took office in January 1969, the Apollo program was already well underway, a result of Kennedy's ambitious goal to land a man on the moon before the end of the decade. Nixon inherited the program at its climax and ensured its success was celebrated as a national achievement. He famously spoke to astronauts Neil Armstrong and Buzz Aldrin during their time on the lunar surface, calling it \"the greatest week in the history of the world since the Creation.\"\n",
      "- 2. Determine who was President of the United States at that time (July-August 1969).: The U.S. president during the historic Apollo 11 moon landing on July 20, 1969, was Richard Nixon. Although the groundwork for the moon landing was laid by his predecessors, particularly John F. Kennedy and Lyndon B. Johnson, Nixon played a significant role in shaping the narrative and public perception of the event.\n",
      "- 3. Find out what policies or initiatives related to space exploration were implemented by this president during his tenure around August-September 1969.: ### Nixon's Space Exploration Policy\n",
      "Nixon's approach to space exploration was more pragmatic compared to Kennedy's visionary stance. While Kennedy had championed the space race as a Cold War imperative, Nixon focused on balancing the costs of space exploration with other national priorities. Under his administration, NASA's budget began to decline, and the Apollo program was scaled back after Apollo 17 in 1972.\n",
      "- Summarize this information:: ### Nixon's Role in the Moon Landing\n",
      "When Nixon took office in January 1969, the Apollo program was already well underway, a result of Kennedy's ambitious goal to land a man on the moon before the end of the decade. Nixon inherited the program at its climax and ensured its success was celebrated as a national achievement. He famously spoke to astronauts Neil Armstrong and Buzz Aldrin during their time on the lunar surface, calling it \"the greatest week in the history of the world since the Creation.\"\n"
     ]
    }
   ],
   "source": [
    "# --- Step 7: Lookup each reasoning step with vectorstore ---\n",
    "# For each reasoning step, perform a similarity search in the vectorstore.\n",
    "for step in step_lines:\n",
    "    print(f\"\\nüîç Looking up: {step}\")\n",
    "    \n",
    "    # Retrieve the most relevant document from the vectorstore.\n",
    "    docs = vectorstore.similarity_search(step, k=1)\n",
    "    result = docs[0].page_content if docs else \"No relevant info found in local knowledge base.\"\n",
    "    \n",
    "    # Append the result to the list of facts.\n",
    "    facts.append(f\"- {step.strip()}: {result}\")\n",
    "\n",
    "# Combine all retrieved facts into a single string.\n",
    "combined_facts = \"\\n\".join(facts)\n",
    "\n",
    "# Print the retrieved facts.\n",
    "print(\"\\nüìö Retrieved facts from Vectorstore:\\n\")\n",
    "print(combined_facts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc91cad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Final synthesized answer:\n",
      "\n",
      "During the historic Apollo 11 moon landing in July 1969, the U.S. president of the United States was Richard Nixon. His policy on space exploration was more pragmatic than Kennedy‚Äôs, focusing on balancing space exploration with other national priorities. Under his administration, NASA‚Äôs budget began to decline, and the Apollo program was scaled back after Apollo 17 in 1972.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 8: Summarize using second LLM ---\n",
    "# Use the synthesis chain to generate a final answer based on the question and retrieved facts.\n",
    "final_answer = synthesis_chain.invoke({\n",
    "    \"question\": question,\n",
    "    \"facts\": combined_facts\n",
    "})\n",
    "\n",
    "# Print the final synthesized answer.\n",
    "print(\"\\n‚úÖ Final synthesized answer:\\n\")\n",
    "print(final_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
