{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "457364d8",
   "metadata": {},
   "source": [
    "# Examples of chunking text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4bfb35",
   "metadata": {},
   "source": [
    "The topic we're discussing is about using a \"text splitter\" in programming, specifically to break down a large piece of text into smaller, manageable chunks. This is particularly useful when dealing with long documents, like Dr. Martin Luther King Jr.'s \"I Have a Dream\" speech, which can be overwhelming to process all at once.\n",
    "\n",
    "Imagine you have a big cake, and you want to share it with your friends. Instead of giving them the whole cake at once, you cut it into slices. Each slice is easier to handle and enjoy. Similarly, a text splitter takes a long text and divides it into smaller sections, or \"chunks,\" making it easier to analyze or work with. In our example, we set the size of each chunk to 100 characters, with a little overlap of 20 characters to ensure we don’t miss any important context between the chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a479e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# Step 1: Load the input text from the specified file path\n",
    "file_path = \"C:\\\\Python\\\\Agent-School\\\\docs\\\\i-have-a-dream.txt\"\n",
    "\n",
    "with open(file_path, encoding=\"utf-8\") as file:\n",
    "    speech = file.read()\n",
    "\n",
    "# Step 2: Split the text into manageable chunks\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=100,     # Max characters per chunk\n",
    "    chunk_overlap=20,   # Overlap between chunks\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "documents = text_splitter.create_documents([speech])\n",
    "print(\"First document chunk:\\n\", documents[0], \"\\n\")\n",
    "# print(documents[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85418929",
   "metadata": {},
   "source": [
    "This code demonstrates how to process text by splitting it into smaller chunks using the `RecursiveCharacterTextSplitter` from the LangChain library. Here's a summary:\n",
    "\n",
    "1. **Load a Speech Text File**: \n",
    "   - It reads the full text of Martin Luther King Jr.'s \"I Have a Dream\" speech from a file located on your computer.\n",
    "\n",
    "2. **Set Up the Text Splitter**:\n",
    "   - A `RecursiveCharacterTextSplitter` object is created with specific parameters:\n",
    "     - **`chunk_size`**: The maximum number of characters per chunk is set to 40.\n",
    "     - **`chunk_overlap`**: Chunks overlap by 12 characters to maintain context between them.\n",
    "     - **`length_function`**: Measures the size of chunks based on character count.\n",
    "     - **`add_start_index`**: Includes the starting index of each chunk for reference.\n",
    "\n",
    "3. **Create Document Objects**:\n",
    "   - The speech text is divided into chunks (following the splitter's configuration), and these chunks are stored as LangChain Document objects.\n",
    "\n",
    "4. **Optional Debug Output**:\n",
    "   - It prints the number of chunks created and previews the first two chunks for verification.\n",
    "\n",
    "5. **Additional Text Splitting Example**:\n",
    "   - A standalone string (\"Python can be easy to pick up...\") is split using the same splitter to illustrate its usage on general text.\n",
    "\n",
    "\n",
    "The last part illustrates nicel what's going on when you are chunking, as it splits up the sentence into a few words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca2a7e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392\n",
      "Doc 1: page_content='﻿As far as black Americans were' metadata={'start_index': 0}\n",
      "Doc 2: page_content='were concerned, the nation’s response' metadata={'start_index': 27}\n",
      "['Python can be easy to pick up whether', \"up whether you're a professional or a\", 'or a beginner.']\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Step 1: Load the raw speech text from the specified file path\n",
    "file_path = \"C:\\\\Python\\\\Agent-School\\\\docs\\\\i-have-a-dream.txt\"\n",
    "with open(file_path, encoding=\"utf-8\") as paper:\n",
    "    speech = paper.read()\n",
    "\n",
    "# Step 2: Split the text using RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=40,         # Max characters per chunk\n",
    "    chunk_overlap=12,      # Overlap between chunks\n",
    "    length_function=len,   # Use length of string to measure size\n",
    "    add_start_index=True   # Track start index of each chunk in the original text\n",
    ")\n",
    "\n",
    "# Step 3: Create LangChain Document objects from the full speech\n",
    "docs = text_splitter.create_documents([speech])\n",
    "\n",
    "# (Optional debug output)\n",
    "print(len(docs))\n",
    "print(f\"Doc 1: {docs[0]}\")\n",
    "print(f\"Doc 2: {docs[1]}\")\n",
    "\n",
    "# Step 4: You can also split any standalone string using the same splitter\n",
    "s = \"Python can be easy to pick up whether you're a professional or a beginner.\"\n",
    "text = text_splitter.split_text(s)\n",
    "print(text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
